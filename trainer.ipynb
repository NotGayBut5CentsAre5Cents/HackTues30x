{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c3a162a-2e9a-459a-bc91-3a35008586b0",
    "_uuid": "355c54dccfd0e1dfd44e342f7ea8fa31353dce11"
   },
   "source": [
    "# Basic Seq2Seq\n",
    "This is a basic seq2seq implementation to show what can be done for conversational models.  The task we'll train it on is predicting company responses to consumers.\n",
    "\n",
    "This notebook shows how to prepare the data and construct the Keras model, but will not train quickly!  Instead, it demonstrates how the network progresses toward natural responses, and allows replying to arbitrary text, as shown below.  Unfortunately, getting to interesting results takes longer than an hour on Kaggle's non-GPU notebooks, so you'll need to download the notebook and run on your own machine to get to interesting results.\n",
    "\n",
    "This configuration tops out at a test loss of ~1.8, and provides nuanced responses to some of the more requests, like \"[the I problem](http://www.refinery29.com/2017/11/179790/ios-11-1-bug-keyboard-problem)\" for @AppleSupport, after around 6 hours of training on a CUDA 5.0 GPU.\n",
    "\n",
    "![seq2seq model architecture](https://i.imgur.com/JmuryKu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "39717eda-bc31-4143-a87f-bac4eea63678",
    "_uuid": "e30b174032d05dd0048b02ca268f3b31cf5b183e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "# print('Library versions:')\n",
    "\n",
    "import keras\n",
    "# print(f'keras:{keras.__version__}')\n",
    "#import pandas as pd\n",
    "#print(f'pandas:{pd.__version__}')\n",
    "import sklearn\n",
    "# print(f'sklearn:{sklearn.__version__}')\n",
    "import nltk\n",
    "# print(f'nltk:{nltk.__version__}')\n",
    "import numpy as np\n",
    "# print(f'numpy:{np.__version__}')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "\n",
    "#from tqdm import tqdm_notebook as tqdm # Special jupyter notebook progress bar ðŸ’«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24fb3871-ea0e-4b9e-b21c-ed76ccefc823",
    "_uuid": "ec0199c0dff22aeed45b9dbe1e7bf659e9449fe9"
   },
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "3217b16f-bf7c-4fed-bbea-d74e09a537e4",
    "_uuid": "6509f5443163b0d11f1a003bb8a28b79a340f192"
   },
   "outputs": [],
   "source": [
    "# 8192 - large enough for demonstration, larger values make network training slower\n",
    "MAX_VOCAB_SIZE = 394\n",
    "# seq2seq generally relies on fixed length message vectors - longer messages provide more info\n",
    "# but result in slower training and larger networks\n",
    "MAX_MESSAGE_LEN = 20 \n",
    "# Embedding size for words - gives a trade off between expressivity of words and network size\n",
    "EMBEDDING_SIZE = 100\n",
    "# Embedding size for whole messages, same trade off as word embeddings\n",
    "CONTEXT_SIZE = 100\n",
    "# Larger batch sizes generally reach the average response faster, but small batch sizes are\n",
    "# required for the model to learn nuanced responses.  Also, GPU memory limits max batch size.\n",
    "BATCH_SIZE = 100\n",
    "# Helps regularize network and prevent overfitting.\n",
    "DROPOUT = 0.2\n",
    "# High learning rate helps model reach average response faster, but can make it hard to \n",
    "# converge on nuanced responses\n",
    "LEARNING_RATE=0.005\n",
    "\n",
    "# Tokens needed for seq2seq\n",
    "UNK = 1  # words that aren't found in the vocab\n",
    "PAD = 0  # after message has finished, this fills all remaining vector positions\n",
    "START = 2  # provided to the model at position 0 for every response predicted\n",
    "\n",
    "# Implementaiton detail for allowing this to be run in Kaggle's notebook hardware\n",
    "SUB_BATCH_SIZE = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea67c840-2d02-45df-be0b-ec5746e72028",
    "_uuid": "9f99990233daf26e624d6e0a735b8a3559eb54c9"
   },
   "source": [
    "## Data Prep\n",
    "Here, we'll prepare the data for training our seq2seq model, including:\n",
    "\n",
    "- Replace screen names with `@__sn__` token to show model the commonality between them\n",
    "- Build a vocab to turn tokens into integers suitable for our seq2seq model\n",
    "- Tokenize input and target text into fixed size vectors\n",
    "- Partition our dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e83096b-f411-494a-962b-80a76122e674",
    "_uuid": "8989bdc2a930380aa52af0ef6a296906d313715c"
   },
   "source": [
    "### Data Loading and Reshaping\n",
    "Pulled from [this kernel](https://www.kaggle.com/soaxelbrooke/first-inbound-and-response-tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0afc1aa-a34f-4ea5-b34e-2fb21b5da62e",
    "_uuid": "2478590bbc0924cc8e8dd127ac4fea8019f8b3f3"
   },
   "source": [
    "### Tokenizing and Vocab Build\n",
    "\n",
    "We'll use NLTK's `casual_tokenize`, which handles a lot of corner cases found in social media data (\"casual\" text data) along with scitkit learn's `CountVectorizer`.  We won't use the actual `CountVectorizer`, just use it as a convenient vocabulary builder, which we'll apply with functions that turn text into \"word indexes\" - integers that represent each word - and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/embeddings.pkl', 'rb') as fp:\n",
    "    our_embedding , idx2word , word2idx = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 274,\n",
       " '%': 28,\n",
       " \"'\": 112,\n",
       " \"''\": 336,\n",
       " '(': 53,\n",
       " ')': 304,\n",
       " ',': 307,\n",
       " '-': 72,\n",
       " '.': 96,\n",
       " '1': 31,\n",
       " '10': 73,\n",
       " '11': 47,\n",
       " '1bn': 315,\n",
       " '2': 4,\n",
       " '2000': 285,\n",
       " '2003': 177,\n",
       " '2005': 157,\n",
       " '27': 281,\n",
       " '3': 358,\n",
       " '300m': 42,\n",
       " '4': 212,\n",
       " '42': 43,\n",
       " '464,000': 291,\n",
       " '5': 361,\n",
       " '500m': 127,\n",
       " '6': 60,\n",
       " '76': 115,\n",
       " '8': 256,\n",
       " '``': 16,\n",
       " 'a': 344,\n",
       " 'about': 152,\n",
       " 'account': 189,\n",
       " 'accounts': 297,\n",
       " 'ad': 54,\n",
       " 'adjust': 169,\n",
       " 'advert': 18,\n",
       " 'advertising': 235,\n",
       " 'after': 260,\n",
       " 'against': 278,\n",
       " 'ahead': 91,\n",
       " 'alan': 233,\n",
       " 'alexander': 142,\n",
       " 'all': 343,\n",
       " 'almost': 206,\n",
       " 'already': 249,\n",
       " 'also': 167,\n",
       " 'america': 331,\n",
       " 'amount': 313,\n",
       " 'an': 253,\n",
       " 'analysts': 84,\n",
       " 'and': 119,\n",
       " 'announce': 20,\n",
       " 'any': 82,\n",
       " 'aol': 373,\n",
       " 'around': 355,\n",
       " 'as': 371,\n",
       " 'aside': 26,\n",
       " 'assets': 368,\n",
       " 'at': 49,\n",
       " 'attractive': 130,\n",
       " 'back': 365,\n",
       " 'bank': 6,\n",
       " 'be': 29,\n",
       " 'been': 237,\n",
       " 'before': 302,\n",
       " 'beijing': 387,\n",
       " 'believe': 74,\n",
       " 'benefited': 124,\n",
       " 'bertelsmann': 80,\n",
       " 'better': 254,\n",
       " 'big': 81,\n",
       " 'biggest': 200,\n",
       " 'bonds': 50,\n",
       " 'book': 363,\n",
       " 'boost': 239,\n",
       " 'boosted': 77,\n",
       " 'both': 284,\n",
       " 'box-office': 277,\n",
       " 'broadband': 94,\n",
       " 'bros': 259,\n",
       " 'budget': 86,\n",
       " 'buoyed': 314,\n",
       " 'business': 353,\n",
       " 'but': 150,\n",
       " 'buying': 333,\n",
       " 'by': 198,\n",
       " 'calls': 225,\n",
       " 'can': 52,\n",
       " 'catwoman': 105,\n",
       " 'chairman': 392,\n",
       " 'charges': 79,\n",
       " 'chief': 366,\n",
       " 'china': 375,\n",
       " 'chinese': 337,\n",
       " 'close': 220,\n",
       " 'commentators': 113,\n",
       " 'comments': 279,\n",
       " 'commission': 386,\n",
       " 'company': 383,\n",
       " 'competitive': 140,\n",
       " 'concerns': 222,\n",
       " 'concluding': 224,\n",
       " 'conditions': 139,\n",
       " 'connections': 217,\n",
       " 'contrast': 207,\n",
       " 'could': 158,\n",
       " 'curb': 312,\n",
       " 'currency': 184,\n",
       " 'current': 287,\n",
       " 'customers': 192,\n",
       " 'data': 159,\n",
       " 'deaf': 160,\n",
       " 'deal': 59,\n",
       " 'december': 250,\n",
       " 'decision': 178,\n",
       " 'deficit': 309,\n",
       " 'deficits': 145,\n",
       " 'despite': 19,\n",
       " 'differential': 243,\n",
       " 'dip': 321,\n",
       " 'division': 12,\n",
       " 'do': 382,\n",
       " 'dollar': 219,\n",
       " 'dollars': 252,\n",
       " 'earlier': 319,\n",
       " 'earnings': 40,\n",
       " 'ears': 226,\n",
       " 'efforts': 223,\n",
       " 'enhancing': 385,\n",
       " 'enough': 194,\n",
       " 'estimate': 171,\n",
       " 'euro': 186,\n",
       " 'europe': 202,\n",
       " 'european': 393,\n",
       " 'exceeding': 316,\n",
       " 'exceptional': 210,\n",
       " 'exchange': 170,\n",
       " 'executive': 99,\n",
       " 'existing': 378,\n",
       " 'expectations': 289,\n",
       " 'expects': 351,\n",
       " 'export': 22,\n",
       " 'factors': 215,\n",
       " 'fallen': 154,\n",
       " 'falls': 65,\n",
       " 'february': 282,\n",
       " 'federal': 270,\n",
       " 'film': 244,\n",
       " 'final': 251,\n",
       " 'finance': 176,\n",
       " 'financial': 62,\n",
       " 'firm': 41,\n",
       " 'firms': 339,\n",
       " 'flexibility': 352,\n",
       " 'flops': 213,\n",
       " 'following': 153,\n",
       " 'for': 197,\n",
       " 'foreign': 114,\n",
       " 'fortunes': 229,\n",
       " 'fourth': 182,\n",
       " 'free': 44,\n",
       " 'friday': 308,\n",
       " 'from': 295,\n",
       " 'full-year': 181,\n",
       " 'funded': 147,\n",
       " 'g7': 125,\n",
       " 'gains': 83,\n",
       " 'gap': 103,\n",
       " 'german': 195,\n",
       " 'giant': 364,\n",
       " 'google': 100,\n",
       " 'government': 349,\n",
       " 'governments': 109,\n",
       " 'greatly': 164,\n",
       " 'greenback': 389,\n",
       " 'greenspan': 148,\n",
       " 'grew': 370,\n",
       " 'growth': 120,\n",
       " 'had': 71,\n",
       " 'half': 292,\n",
       " 'half-point': 162,\n",
       " 'has': 288,\n",
       " 'have': 76,\n",
       " 'he': 38,\n",
       " 'head': 231,\n",
       " 'help': 149,\n",
       " 'helped': 255,\n",
       " 'high-speed': 133,\n",
       " 'higher': 5,\n",
       " 'highest': 67,\n",
       " 'highlighted': 135,\n",
       " 'highly': 384,\n",
       " 'hit': 15,\n",
       " 'hopes': 265,\n",
       " 'house': 89,\n",
       " 'household': 185,\n",
       " 'however': 238,\n",
       " 'i': 39,\n",
       " 'improve': 205,\n",
       " 'in': 107,\n",
       " 'increase': 56,\n",
       " 'inquiry': 208,\n",
       " 'intends': 172,\n",
       " 'interest': 68,\n",
       " 'internet': 340,\n",
       " 'into': 326,\n",
       " 'investors': 111,\n",
       " 'is': 187,\n",
       " 'it': 196,\n",
       " 'items': 90,\n",
       " 'its': 88,\n",
       " 'jobs': 98,\n",
       " 'jumped': 261,\n",
       " 'keep': 35,\n",
       " 'late': 134,\n",
       " 'laying': 341,\n",
       " 'legal': 381,\n",
       " 'less': 377,\n",
       " 'level': 69,\n",
       " 'london': 104,\n",
       " 'longer-term': 168,\n",
       " 'looking': 273,\n",
       " 'loosening': 280,\n",
       " 'lord': 199,\n",
       " 'loss': 303,\n",
       " 'lost': 23,\n",
       " 'lower': 374,\n",
       " 'made': 143,\n",
       " 'major': 390,\n",
       " 'many': 299,\n",
       " 'margins': 263,\n",
       " 'market': 131,\n",
       " 'may': 151,\n",
       " 'meaningful': 21,\n",
       " 'meantime': 320,\n",
       " 'media': 92,\n",
       " 'meeting': 33,\n",
       " 'ministers': 311,\n",
       " 'mixed': 78,\n",
       " 'monday': 346,\n",
       " 'months': 376,\n",
       " 'more': 245,\n",
       " 'move': 63,\n",
       " 'movement': 323,\n",
       " 'mr': 183,\n",
       " 'much': 211,\n",
       " 'music': 294,\n",
       " 'need': 106,\n",
       " 'needed': 11,\n",
       " 'new': 264,\n",
       " 'newspaper': 7,\n",
       " 'next': 156,\n",
       " 'now': 173,\n",
       " 'objectives': 276,\n",
       " 'of': 350,\n",
       " 'offered': 241,\n",
       " 'offering': 372,\n",
       " 'offset': 27,\n",
       " 'on': 93,\n",
       " 'one': 97,\n",
       " 'one-off': 262,\n",
       " 'online': 379,\n",
       " 'opened': 163,\n",
       " 'operating': 95,\n",
       " 'or': 329,\n",
       " 'our': 190,\n",
       " 'out': 347,\n",
       " 'own': 25,\n",
       " 'owns': 242,\n",
       " 'parsons': 394,\n",
       " 'part': 359,\n",
       " 'partly': 306,\n",
       " 'pay': 126,\n",
       " 'peg': 283,\n",
       " 'pegged': 161,\n",
       " 'performance': 228,\n",
       " 'point': 132,\n",
       " 'policy': 46,\n",
       " 'posted': 356,\n",
       " 'preceding': 45,\n",
       " 'previously': 266,\n",
       " 'prices': 128,\n",
       " 'probe': 293,\n",
       " 'produce': 9,\n",
       " 'profit': 70,\n",
       " 'profits': 209,\n",
       " 'projecting': 8,\n",
       " 'prop': 342,\n",
       " 'publisher': 290,\n",
       " 'purchase': 246,\n",
       " 'quarter': 204,\n",
       " 'quarterly': 116,\n",
       " 'quarters': 305,\n",
       " 'rates': 247,\n",
       " 'reached': 66,\n",
       " 'recent': 13,\n",
       " 'reduce': 332,\n",
       " 'regulators': 179,\n",
       " 'remain': 118,\n",
       " 'remains': 85,\n",
       " 'reported': 354,\n",
       " 'reserve': 191,\n",
       " 'reserves': 17,\n",
       " 'resolve': 216,\n",
       " 'restate': 144,\n",
       " 'result': 230,\n",
       " 'results': 227,\n",
       " 'revenue': 117,\n",
       " 'revenues': 240,\n",
       " 'review': 301,\n",
       " 'richard': 137,\n",
       " 'rings': 221,\n",
       " 'ripe': 61,\n",
       " 'rising': 129,\n",
       " 'robert': 201,\n",
       " 'rose': 188,\n",
       " 's': 271,\n",
       " 'said': 75,\n",
       " 'sale': 395,\n",
       " 'sales': 141,\n",
       " 'sanguine': 136,\n",
       " 'savings': 146,\n",
       " 'saw': 248,\n",
       " 'sec': 236,\n",
       " 'securities': 232,\n",
       " 'sent': 122,\n",
       " 'service': 174,\n",
       " 'set': 258,\n",
       " 'settle': 24,\n",
       " 'sharp': 272,\n",
       " 'shift': 180,\n",
       " 'sign': 110,\n",
       " 'sinche': 55,\n",
       " 'sixth': 57,\n",
       " 'slightly': 330,\n",
       " 'slump': 380,\n",
       " 'some': 286,\n",
       " 'speech': 123,\n",
       " 'spending': 327,\n",
       " 'stabilise': 193,\n",
       " 'stake': 51,\n",
       " 'strategy': 102,\n",
       " 'strong': 360,\n",
       " 'stronger': 101,\n",
       " 'subscribers': 36,\n",
       " 'such': 275,\n",
       " 'taken': 396,\n",
       " 'taking': 218,\n",
       " 'than': 362,\n",
       " 'that': 203,\n",
       " 'the': 166,\n",
       " 'therefore': 87,\n",
       " 'think': 298,\n",
       " 'third': 338,\n",
       " 'this': 30,\n",
       " 'thought': 3,\n",
       " 'three': 369,\n",
       " 'thursday': 64,\n",
       " 'time': 214,\n",
       " 'timewarner': 10,\n",
       " 'to': 32,\n",
       " 'trade': 267,\n",
       " 'trading': 322,\n",
       " 'trillion': 37,\n",
       " 'trilogy': 175,\n",
       " 'try': 121,\n",
       " 'tumbled': 334,\n",
       " 'unable': 367,\n",
       " 'under': 234,\n",
       " 'underlying': 155,\n",
       " 'unlikely': 357,\n",
       " 'up': 318,\n",
       " 'us': 268,\n",
       " 'users': 328,\n",
       " 'value': 296,\n",
       " 'view': 58,\n",
       " 'warner': 345,\n",
       " 'was': 317,\n",
       " 'way': 14,\n",
       " 'well': 388,\n",
       " 'were': 269,\n",
       " 'when': 138,\n",
       " 'which': 108,\n",
       " 'while': 335,\n",
       " 'white': 324,\n",
       " 'wider': 257,\n",
       " 'will': 300,\n",
       " 'willingness': 325,\n",
       " 'window': 48,\n",
       " 'with': 165,\n",
       " 'worries': 391,\n",
       " 'yawning': 34,\n",
       " 'year': 348,\n",
       " 'york': 310}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 'thought',\n",
       " 4: '2',\n",
       " 5: 'higher',\n",
       " 6: 'bank',\n",
       " 7: 'newspaper',\n",
       " 8: 'projecting',\n",
       " 9: 'produce',\n",
       " 10: 'timewarner',\n",
       " 11: 'needed',\n",
       " 12: 'division',\n",
       " 13: 'recent',\n",
       " 14: 'way',\n",
       " 15: 'hit',\n",
       " 16: '``',\n",
       " 17: 'reserves',\n",
       " 18: 'advert',\n",
       " 19: 'despite',\n",
       " 20: 'announce',\n",
       " 21: 'meaningful',\n",
       " 22: 'export',\n",
       " 23: 'lost',\n",
       " 24: 'settle',\n",
       " 25: 'own',\n",
       " 26: 'aside',\n",
       " 27: 'offset',\n",
       " 28: '%',\n",
       " 29: 'be',\n",
       " 30: 'this',\n",
       " 31: '1',\n",
       " 32: 'to',\n",
       " 33: 'meeting',\n",
       " 34: 'yawning',\n",
       " 35: 'keep',\n",
       " 36: 'subscribers',\n",
       " 37: 'trillion',\n",
       " 38: 'he',\n",
       " 39: 'i',\n",
       " 40: 'earnings',\n",
       " 41: 'firm',\n",
       " 42: '300m',\n",
       " 43: '42',\n",
       " 44: 'free',\n",
       " 45: 'preceding',\n",
       " 46: 'policy',\n",
       " 47: '11',\n",
       " 48: 'window',\n",
       " 49: 'at',\n",
       " 50: 'bonds',\n",
       " 51: 'stake',\n",
       " 52: 'can',\n",
       " 53: '(',\n",
       " 54: 'ad',\n",
       " 55: 'sinche',\n",
       " 56: 'increase',\n",
       " 57: 'sixth',\n",
       " 58: 'view',\n",
       " 59: 'deal',\n",
       " 60: '6',\n",
       " 61: 'ripe',\n",
       " 62: 'financial',\n",
       " 63: 'move',\n",
       " 64: 'thursday',\n",
       " 65: 'falls',\n",
       " 66: 'reached',\n",
       " 67: 'highest',\n",
       " 68: 'interest',\n",
       " 69: 'level',\n",
       " 70: 'profit',\n",
       " 71: 'had',\n",
       " 72: '-',\n",
       " 73: '10',\n",
       " 74: 'believe',\n",
       " 75: 'said',\n",
       " 76: 'have',\n",
       " 77: 'boosted',\n",
       " 78: 'mixed',\n",
       " 79: 'charges',\n",
       " 80: 'bertelsmann',\n",
       " 81: 'big',\n",
       " 82: 'any',\n",
       " 83: 'gains',\n",
       " 84: 'analysts',\n",
       " 85: 'remains',\n",
       " 86: 'budget',\n",
       " 87: 'therefore',\n",
       " 88: 'its',\n",
       " 89: 'house',\n",
       " 90: 'items',\n",
       " 91: 'ahead',\n",
       " 92: 'media',\n",
       " 93: 'on',\n",
       " 94: 'broadband',\n",
       " 95: 'operating',\n",
       " 96: '.',\n",
       " 97: 'one',\n",
       " 98: 'jobs',\n",
       " 99: 'executive',\n",
       " 100: 'google',\n",
       " 101: 'stronger',\n",
       " 102: 'strategy',\n",
       " 103: 'gap',\n",
       " 104: 'london',\n",
       " 105: 'catwoman',\n",
       " 106: 'need',\n",
       " 107: 'in',\n",
       " 108: 'which',\n",
       " 109: 'governments',\n",
       " 110: 'sign',\n",
       " 111: 'investors',\n",
       " 112: \"'\",\n",
       " 113: 'commentators',\n",
       " 114: 'foreign',\n",
       " 115: '76',\n",
       " 116: 'quarterly',\n",
       " 117: 'revenue',\n",
       " 118: 'remain',\n",
       " 119: 'and',\n",
       " 120: 'growth',\n",
       " 121: 'try',\n",
       " 122: 'sent',\n",
       " 123: 'speech',\n",
       " 124: 'benefited',\n",
       " 125: 'g7',\n",
       " 126: 'pay',\n",
       " 127: '500m',\n",
       " 128: 'prices',\n",
       " 129: 'rising',\n",
       " 130: 'attractive',\n",
       " 131: 'market',\n",
       " 132: 'point',\n",
       " 133: 'high-speed',\n",
       " 134: 'late',\n",
       " 135: 'highlighted',\n",
       " 136: 'sanguine',\n",
       " 137: 'richard',\n",
       " 138: 'when',\n",
       " 139: 'conditions',\n",
       " 140: 'competitive',\n",
       " 141: 'sales',\n",
       " 142: 'alexander',\n",
       " 143: 'made',\n",
       " 144: 'restate',\n",
       " 145: 'deficits',\n",
       " 146: 'savings',\n",
       " 147: 'funded',\n",
       " 148: 'greenspan',\n",
       " 149: 'help',\n",
       " 150: 'but',\n",
       " 151: 'may',\n",
       " 152: 'about',\n",
       " 153: 'following',\n",
       " 154: 'fallen',\n",
       " 155: 'underlying',\n",
       " 156: 'next',\n",
       " 157: '2005',\n",
       " 158: 'could',\n",
       " 159: 'data',\n",
       " 160: 'deaf',\n",
       " 161: 'pegged',\n",
       " 162: 'half-point',\n",
       " 163: 'opened',\n",
       " 164: 'greatly',\n",
       " 165: 'with',\n",
       " 166: 'the',\n",
       " 167: 'also',\n",
       " 168: 'longer-term',\n",
       " 169: 'adjust',\n",
       " 170: 'exchange',\n",
       " 171: 'estimate',\n",
       " 172: 'intends',\n",
       " 173: 'now',\n",
       " 174: 'service',\n",
       " 175: 'trilogy',\n",
       " 176: 'finance',\n",
       " 177: '2003',\n",
       " 178: 'decision',\n",
       " 179: 'regulators',\n",
       " 180: 'shift',\n",
       " 181: 'full-year',\n",
       " 182: 'fourth',\n",
       " 183: 'mr',\n",
       " 184: 'currency',\n",
       " 185: 'household',\n",
       " 186: 'euro',\n",
       " 187: 'is',\n",
       " 188: 'rose',\n",
       " 189: 'account',\n",
       " 190: 'our',\n",
       " 191: 'reserve',\n",
       " 192: 'customers',\n",
       " 193: 'stabilise',\n",
       " 194: 'enough',\n",
       " 195: 'german',\n",
       " 196: 'it',\n",
       " 197: 'for',\n",
       " 198: 'by',\n",
       " 199: 'lord',\n",
       " 200: 'biggest',\n",
       " 201: 'robert',\n",
       " 202: 'europe',\n",
       " 203: 'that',\n",
       " 204: 'quarter',\n",
       " 205: 'improve',\n",
       " 206: 'almost',\n",
       " 207: 'contrast',\n",
       " 208: 'inquiry',\n",
       " 209: 'profits',\n",
       " 210: 'exceptional',\n",
       " 211: 'much',\n",
       " 212: '4',\n",
       " 213: 'flops',\n",
       " 214: 'time',\n",
       " 215: 'factors',\n",
       " 216: 'resolve',\n",
       " 217: 'connections',\n",
       " 218: 'taking',\n",
       " 219: 'dollar',\n",
       " 220: 'close',\n",
       " 221: 'rings',\n",
       " 222: 'concerns',\n",
       " 223: 'efforts',\n",
       " 224: 'concluding',\n",
       " 225: 'calls',\n",
       " 226: 'ears',\n",
       " 227: 'results',\n",
       " 228: 'performance',\n",
       " 229: 'fortunes',\n",
       " 230: 'result',\n",
       " 231: 'head',\n",
       " 232: 'securities',\n",
       " 233: 'alan',\n",
       " 234: 'under',\n",
       " 235: 'advertising',\n",
       " 236: 'sec',\n",
       " 237: 'been',\n",
       " 238: 'however',\n",
       " 239: 'boost',\n",
       " 240: 'revenues',\n",
       " 241: 'offered',\n",
       " 242: 'owns',\n",
       " 243: 'differential',\n",
       " 244: 'film',\n",
       " 245: 'more',\n",
       " 246: 'purchase',\n",
       " 247: 'rates',\n",
       " 248: 'saw',\n",
       " 249: 'already',\n",
       " 250: 'december',\n",
       " 251: 'final',\n",
       " 252: 'dollars',\n",
       " 253: 'an',\n",
       " 254: 'better',\n",
       " 255: 'helped',\n",
       " 256: '8',\n",
       " 257: 'wider',\n",
       " 258: 'set',\n",
       " 259: 'bros',\n",
       " 260: 'after',\n",
       " 261: 'jumped',\n",
       " 262: 'one-off',\n",
       " 263: 'margins',\n",
       " 264: 'new',\n",
       " 265: 'hopes',\n",
       " 266: 'previously',\n",
       " 267: 'trade',\n",
       " 268: 'us',\n",
       " 269: 'were',\n",
       " 270: 'federal',\n",
       " 271: 's',\n",
       " 272: 'sharp',\n",
       " 273: 'looking',\n",
       " 274: '$',\n",
       " 275: 'such',\n",
       " 276: 'objectives',\n",
       " 277: 'box-office',\n",
       " 278: 'against',\n",
       " 279: 'comments',\n",
       " 280: 'loosening',\n",
       " 281: '27',\n",
       " 282: 'february',\n",
       " 283: 'peg',\n",
       " 284: 'both',\n",
       " 285: '2000',\n",
       " 286: 'some',\n",
       " 287: 'current',\n",
       " 288: 'has',\n",
       " 289: 'expectations',\n",
       " 290: 'publisher',\n",
       " 291: '464,000',\n",
       " 292: 'half',\n",
       " 293: 'probe',\n",
       " 294: 'music',\n",
       " 295: 'from',\n",
       " 296: 'value',\n",
       " 297: 'accounts',\n",
       " 298: 'think',\n",
       " 299: 'many',\n",
       " 300: 'will',\n",
       " 301: 'review',\n",
       " 302: 'before',\n",
       " 303: 'loss',\n",
       " 304: ')',\n",
       " 305: 'quarters',\n",
       " 306: 'partly',\n",
       " 307: ',',\n",
       " 308: 'friday',\n",
       " 309: 'deficit',\n",
       " 310: 'york',\n",
       " 311: 'ministers',\n",
       " 312: 'curb',\n",
       " 313: 'amount',\n",
       " 314: 'buoyed',\n",
       " 315: '1bn',\n",
       " 316: 'exceeding',\n",
       " 317: 'was',\n",
       " 318: 'up',\n",
       " 319: 'earlier',\n",
       " 320: 'meantime',\n",
       " 321: 'dip',\n",
       " 322: 'trading',\n",
       " 323: 'movement',\n",
       " 324: 'white',\n",
       " 325: 'willingness',\n",
       " 326: 'into',\n",
       " 327: 'spending',\n",
       " 328: 'users',\n",
       " 329: 'or',\n",
       " 330: 'slightly',\n",
       " 331: 'america',\n",
       " 332: 'reduce',\n",
       " 333: 'buying',\n",
       " 334: 'tumbled',\n",
       " 335: 'while',\n",
       " 336: \"''\",\n",
       " 337: 'chinese',\n",
       " 338: 'third',\n",
       " 339: 'firms',\n",
       " 340: 'internet',\n",
       " 341: 'laying',\n",
       " 342: 'prop',\n",
       " 343: 'all',\n",
       " 344: 'a',\n",
       " 345: 'warner',\n",
       " 346: 'monday',\n",
       " 347: 'out',\n",
       " 348: 'year',\n",
       " 349: 'government',\n",
       " 350: 'of',\n",
       " 351: 'expects',\n",
       " 352: 'flexibility',\n",
       " 353: 'business',\n",
       " 354: 'reported',\n",
       " 355: 'around',\n",
       " 356: 'posted',\n",
       " 357: 'unlikely',\n",
       " 358: '3',\n",
       " 359: 'part',\n",
       " 360: 'strong',\n",
       " 361: '5',\n",
       " 362: 'than',\n",
       " 363: 'book',\n",
       " 364: 'giant',\n",
       " 365: 'back',\n",
       " 366: 'chief',\n",
       " 367: 'unable',\n",
       " 368: 'assets',\n",
       " 369: 'three',\n",
       " 370: 'grew',\n",
       " 371: 'as',\n",
       " 372: 'offering',\n",
       " 373: 'aol',\n",
       " 374: 'lower',\n",
       " 375: 'china',\n",
       " 376: 'months',\n",
       " 377: 'less',\n",
       " 378: 'existing',\n",
       " 379: 'online',\n",
       " 380: 'slump',\n",
       " 381: 'legal',\n",
       " 382: 'do',\n",
       " 383: 'company',\n",
       " 384: 'highly',\n",
       " 385: 'enhancing',\n",
       " 386: 'commission',\n",
       " 387: 'beijing',\n",
       " 388: 'well',\n",
       " 389: 'greenback',\n",
       " 390: 'major',\n",
       " 391: 'worries',\n",
       " 392: 'chairman',\n",
       " 393: 'european',\n",
       " 394: 'parsons',\n",
       " 395: 'sale',\n",
       " 396: 'taken'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['__unk__'] = UNK\n",
    "word2idx['__pad__'] = PAD\n",
    "word2idx['__start__'] = START\n",
    "\n",
    "idx2word[UNK] = '__unk__'\n",
    "idx2word[PAD] = '__pad__'\n",
    "idx2word[START] =  '__start__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/xy.pkl', 'rb') as fp:\n",
    "    x, y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[54, 141, 239, 214, 345, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [219, 83, 93, 148, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 54, 141, 239, 214, 345,  70,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [219,  83,  93, 148, 123,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c01e85bb-974d-4bf3-8eb1-b5893ce8f284",
    "_uuid": "893cae536132c7477699350574f5de0fd7a60a3f"
   },
   "source": [
    "### Vocab Helper Functions\n",
    "These helper functions take strings and turn them into word indexes used by the actual seq2seq models.  This turns something like \"This is how we do it.\" into a padded array of integers, like [153, 4, 643, 48, 94, 54, 8, 0, 0, 0].  We'll apply the `to_word_idx` function to our text data to get our `N x MESSAGE_LEN` training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pretokenize(sentence):\n",
    "    chars = r'([\\.\\'\"])'\n",
    "    return re.sub(chars, r' \\1 ', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "d04887da-5ca9-4380-949e-7f7923a85df8",
    "_uuid": "b32c76052be9f5493b7eeee1cf40435e438eb3ca"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def to_word_idx(sentence):\n",
    "    full_length = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(pretokenize(sentence))] + [0] * 20\n",
    "    return full_length[:20]\n",
    "\n",
    "def from_word_idx(word_idxs):\n",
    "    return ' '.join(idx2word[idx] for idx in word_idxs if idx != PAD).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quarterly profits at us media giant timewarner jumped 76 % to $ 1 . __unk__ ( __unk__ ) for the'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_word_idx(to_word_idx('Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Ð’Ðˆ600m) for the three months to December, from $639m year-earlier.The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL\\'s underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL\\'s existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.Time Warner\\'s fourth quarter profits were slightly better than analysts\\' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann\\'s purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edebf7ab-0495-4e38-9900-7a9652cdb2d0",
    "_uuid": "f9f842020ecb321805c9032d20b01f1ffe6f5885"
   },
   "source": [
    "### Train / Test Split\n",
    "Here, we split our data into training and test sets.  For simplicity, we use a random split, which may result in different distributions between the training and test set, but we won't worry about that for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "67b9f6e4-2619-48c9-9339-da6637e646d3",
    "_uuid": "d4f3b7134ac395a1952f8dc816cedad201cdf0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of shape {2} and test data of shape {20}.\n"
     ]
    }
   ],
   "source": [
    "all_idx = list(range(len(x)))\n",
    "train_idx = set(random.sample(all_idx, int(0.8 * len(all_idx))))\n",
    "test_idx = {idx for idx in all_idx if idx not in train_idx}\n",
    "\n",
    "train_x = x[:]#[list(train_idx)]\n",
    "test_x = x[:]#[list(test_idx)]\n",
    "train_y = y[:]#[list(train_idx)]\n",
    "test_y = y[:]#[list(test_idx)]\n",
    "\n",
    "assert train_x.shape == train_y.shape\n",
    "assert test_x.shape == test_y.shape\n",
    "\n",
    "print('Training data of shape {%d} and test data of shape {%d}.'%train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9764b6aa-13e3-409e-939e-64f7f7083350",
    "_uuid": "89c827fa1a1aabc990e6b04b4a4839bd04b9b42c"
   },
   "source": [
    "## Model Creation\n",
    "We'll create and compile the model here.  It will consist of the following components:\n",
    "\n",
    "- Shared word embeddings\n",
    "  - A shared embedding layer that turns word indexes (a sparse representation) into a dense/compressed representation.  This embeds both the request from the customer, and also the last words uttered by the model that are fed back into the model.\n",
    "- Encoder RNN\n",
    "  - In this case, a single LSTM layer.  This encodes the whole input sentence into a context vector (or thought vector) that represents completely what the customer is saying, and produces a single output.\n",
    "- Decoder RNN\n",
    "  - This RNN (also an LSTM in this case) decodes the context vector into a string of tokens/utterances.  For each time step, it takes the context vector and the embedded last utterance and produces the next utterance, which is fed back into the model.  More complex and effective models copy the encoder state into the decoder, add more layers of LSTMs, and apply attention mechanisms - but these are out of the scope of this simple example.\n",
    "- Next Word Dense+Softmax\n",
    "  - These two layers take the decoder output and turn it into the next word to be uttered.  The dense layer allows the decoder to not map directly to words uttered, and the softmax turns the dense layer output into a probability distribution, from which we pick the most likely next word.\n",
    "\n",
    "![seq2seq model structure](https://i.imgur.com/JmuryKu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "ee656365-3b04-4dfc-a4dc-a4081ceea82d",
    "_uuid": "46bea6af349b1b67b870eb5977c2bd0c798e2b86"
   },
   "outputs": [],
   "source": [
    "# keras imports, because there are like... A million of them.\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Embedding, RepeatVector, concatenate, \\\n",
    "    TimeDistributed\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 54, 141, 239, 214, 345,  70,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0],\n",
       "       [219,  83,  93, 148, 123,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "d0a87457-573a-43b8-9a0a-d869a75b86ba",
    "_uuid": "420e809e7f3c9986bf4d3e3804198b8205e4a772"
   },
   "outputs": [],
   "source": [
    " def create_model():\n",
    "    shared_embedding = Embedding(\n",
    "        output_dim=EMBEDDING_SIZE,\n",
    "        input_dim=MAX_VOCAB_SIZE,\n",
    "        input_length=MAX_MESSAGE_LEN,\n",
    "        name='embedding',   \n",
    "    )\n",
    "    # ENCODER\n",
    "    encoder_input = Input(\n",
    "        shape=(MAX_MESSAGE_LEN,),\n",
    "        dtype='int32',\n",
    "        name='encoder_input',\n",
    "    )\n",
    "    \n",
    "    embedded_input = shared_embedding(encoder_input)\n",
    "    \n",
    "    # No return_sequences - since the encoder here only produces a single value for the\n",
    "    # input sequence provided.\n",
    "    encoder_rnn = LSTM(\n",
    "        CONTEXT_SIZE,\n",
    "        name='encoder',\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    context = RepeatVector(MAX_MESSAGE_LEN)(encoder_rnn(embedded_input))\n",
    "    \n",
    "    # DECODER\n",
    "    \n",
    "    last_word_input = Input(\n",
    "        shape=(MAX_MESSAGE_LEN, ),\n",
    "        dtype='int32',\n",
    "        name='last_word_input',\n",
    "    )\n",
    "    \n",
    "    embedded_last_word = shared_embedding(last_word_input)\n",
    "    # Combines the context produced by the encoder and the last word uttered as inputs\n",
    "    # to the decoder.\n",
    "    decoder_input = concatenate([embedded_last_word, context], axis=2)\n",
    "    \n",
    "    # return_sequences causes LSTM to produce one output per timestep instead of one at the\n",
    "    # end of the intput, which is important for sequence producing models.\n",
    "    decoder_rnn = LSTM(\n",
    "        CONTEXT_SIZE,\n",
    "        name='decoder',\n",
    "        return_sequences=True,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    decoder_output = decoder_rnn(decoder_input)\n",
    "    \n",
    "    # TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n",
    "    next_word_dense = TimeDistributed(\n",
    "        Dense(int(MAX_VOCAB_SIZE / 2), activation='relu'),\n",
    "        name='next_word_dense',\n",
    "    )(decoder_output)\n",
    "    \n",
    "    next_word = TimeDistributed(\n",
    "        Dense(MAX_VOCAB_SIZE, activation='softmax'),\n",
    "        name='next_word_softmax'\n",
    "    )(next_word_dense)\n",
    "    \n",
    "    return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n",
    "\n",
    "s2s_model = create_model()\n",
    "optimizer = Adam(lr=LEARNING_RATE, clipvalue=5.0)\n",
    "s2s_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37dc7acc-ed74-4933-9182-b4329258d31f",
    "_uuid": "261fdad98295fa395592a5a6bf60806151a51c74"
   },
   "source": [
    "## Model Training\n",
    "We'll train the model here.  After each sub-batch of the dataset, we'll test with static input strings to see how the model is progressing in human readable terms.  Its important to have these tests along with traditional model evaluation to provide a better understanding of how well the model is training.\n",
    "\n",
    "It's important to pull test strings from the real distribution of the data, also.  It can be hard to really put yourself in customers' shoes when writing test messages, and you will get non-representative results when you provide test examples that don't fit the true distribution of the input data (when your input text doesn't sound like real customer requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "db2eee93-8070-4572-962e-c7225b20d2e4",
    "_uuid": "464355ec34af2c192723e66222c036826ac323f7"
   },
   "outputs": [],
   "source": [
    "def add_start_token(y_array):\n",
    "    \"\"\" Adds the start token to vectors.  Used for training data. \"\"\"\n",
    "    return np.hstack([\n",
    "        START * np.ones((len(y_array), 1)),\n",
    "        y_array[:, :-1],\n",
    "    ])\n",
    "\n",
    "def binarize_labels(labels):\n",
    "    \"\"\" Helper function that turns integer word indexes into sparse binary matrices for \n",
    "        the expected model output.\n",
    "    \"\"\"\n",
    "    return np.array([np_utils.to_categorical(row, num_classes=MAX_VOCAB_SIZE)\n",
    "                     for row in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "ce33ef3c-90df-4d3f-aad7-82db7b8cbf38",
    "_uuid": "982ca948fcf372ba6d29cc1872b9667c8e7c678f"
   },
   "outputs": [],
   "source": [
    "def respond_to(model, text):\n",
    "    \"\"\" Helper function that takes a text input and provides a text output. \"\"\"\n",
    "    input_y = add_start_token(PAD * np.ones((1, MAX_MESSAGE_LEN)))\n",
    "    idxs = np.array(to_word_idx(text)).reshape((1, MAX_MESSAGE_LEN))\n",
    "    for position in range(MAX_MESSAGE_LEN - 1):\n",
    "        prediction = model.predict([idxs, input_y]).argmax(axis=2)[0]\n",
    "        input_y[:,position + 1] = prediction[position]\n",
    "    return from_word_idx(model.predict([idxs, input_y]).argmax(axis=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "3fb77b82-c4c3-4328-89a9-4c679744018c",
    "_uuid": "134fb65418234dd849a6d0147f95728373ea80a0"
   },
   "outputs": [],
   "source": [
    "def train_mini_epoch(model, start_idx, end_idx):\n",
    "    \"\"\" Batching seems necessary in Kaggle Jupyter Notebook environments, since\n",
    "        `model.fit` seems to freeze on larger batches (somewhere 1k-10k).\n",
    "    \"\"\"\n",
    "    b_train_y = binarize_labels(train_y[start_idx:end_idx])\n",
    "    input_train_y = add_start_token(train_y[start_idx:end_idx])\n",
    "    \n",
    "    model.fit(\n",
    "        [train_x[start_idx:end_idx], input_train_y], \n",
    "        b_train_y,\n",
    "        epochs=1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    rand_idx = 0 #random.sample(list(range(len(test_x))), SUB_BATCH_SIZE)\n",
    "    #print('Test results:', model.evaluate(\n",
    "    #    [train_x[rand_idx], add_start_token(train_x[rand_idx])],\n",
    "    #    binarize_labels(test_y[rand_idx])\n",
    "    #))\n",
    "    \n",
    "    input_strings = [\n",
    "        'The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.And Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"Worries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.'    \n",
    "    ]\n",
    "    \n",
    "    for input_string in input_strings:\n",
    "        output_string = respond_to(model, input_string)\n",
    "        print('< \"{%s}\"'%output_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a481fec-429a-45c5-bbb7-5f8f7e0e2487",
    "_uuid": "00e0850fe55d06f6d86c71c055ee45d8a0086c21"
   },
   "source": [
    "### Train the model!\n",
    "\n",
    "You can stop training by pressing the stop button - the training code is configured to watch for the `KeyboardInterrupt` exception triggered that way.  Also, it will run until the configured stopping point below.\n",
    "\n",
    "\n",
    "Let's start the training! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "450d8fd0-25b2-41cb-ad3b-dbb266568b72",
    "_uuid": "790bb2978f320f7988f0e850026f264a434b5c3e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.1328\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1223\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1254\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1119\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1139\n",
      "< \"{ad sales boost time warner}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1743\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1103\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1130\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1367\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1183\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1100\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1030\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1106\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1077\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1039\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1042\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0983\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0993\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1028\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0975\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0911\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1014\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0924\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0904\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0896\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1010\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0935\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0849\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0938\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1221\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0831\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0902\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0812\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0836\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0865\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0765\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0859\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0826\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0877\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0898\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1061\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0768\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0729\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0799\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0838\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0764\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0738\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0731\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0840\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0769\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0680\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0694\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0811\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0675\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0894\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0710\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0719\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0714\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0664\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0646\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0746\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0732\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0776\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0701\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0707\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0686\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0631\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0600\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0600\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0612\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0774\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0661\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0674\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0619\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0599\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0688\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0593\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0608\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0616\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0581\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0722\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0570\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0661\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0616\n",
      "< \"{ad sales boost time warner profit}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0633\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0567\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0550\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0533\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0642\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0848\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0549\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0535\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0522\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0539\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0563\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0551\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0666\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0515\n",
      "< \"{dollar gains on greenspan speech}\"\n",
      "Training in epoch {epoch}...\n",
      "Epoch 1/1\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0506\n",
      "< \"{dollar gains on greenspan speech}\"\n"
     ]
    }
   ],
   "source": [
    "training_time_limit = 360 * 60  # seconds (notebooks terminate after 1 hour)\n",
    "start_time = time.time()\n",
    "stop_after = start_time + training_time_limit\n",
    "\n",
    "class TimesUpInterrupt(Exception):\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    for epoch in range(100):\n",
    "        print('Training in epoch {epoch}...')\n",
    "        for start_idx in range(0, len(train_x), SUB_BATCH_SIZE):\n",
    "            train_mini_epoch(s2s_model, start_idx, start_idx + SUB_BATCH_SIZE)\n",
    "            if time.time() > stop_after:\n",
    "                raise TimesUpInterrupt\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Halting training from keyboard interrupt.\")\n",
    "except TimesUpInterrupt:\n",
    "    print(\"Halting after {time.time() - start_time} seconds spent training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "18a421a1-41d7-4c87-988d-74fa8dc85992",
    "_uuid": "8ebd6c01cd3a7e75810f9fc48b4d2ad913f9f410"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dollar gains on greenspan speech'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respond_to(s2s_model,'The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.And Alan Greenspan highlighted the US government\\'s willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan\\'s speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman\\'s taking a much more sanguine view on the current account deficit than he\\'s taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He\\'s taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"Worries about the deficit concerns about China do, however, remain. China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing\\'s policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve\\'s decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US\\'s yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "1445794c-c26f-4dc9-a38e-90b98ee206cb",
    "_uuid": "f7e4303f8ee7d2def95c95b166b6fac8727217f5"
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s_model.save(\"./models/s2s.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
